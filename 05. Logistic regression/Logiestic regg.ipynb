{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import needed packages\n",
    "# You may add or remove packages should you need them\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn import model_selection\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LogisticRegressions:\n",
    "    def __init__(self, learning_rate=0.001, iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iterations = iterations\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        \n",
    "    def _sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))   \n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # init parameters\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        # gradient descent\n",
    "        for i in range(self.iterations):\n",
    "            # approximate y with linear combination of weights and x, plus bias\n",
    "            # apply sigmoid function\n",
    "            y_predicted = self._sigmoid(np.dot(X, self.weights) + self.bias)#y=mx+c\n",
    "\n",
    "            # Cost function\n",
    "            cost = -(1/n_features)*np.sum(y*np.log(y_predicted)+(1-y)*np.log(1-y_predicted))\n",
    "\n",
    "            # compute gradients\n",
    "            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y)) \n",
    "            db = (1 / n_samples) * np.sum(y_predicted - y)\n",
    "            # update parameters\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        linear_model = np.dot(X, self.weights) + self.bias\n",
    "        y_predicted = self._sigmoid(linear_model)\n",
    "        y_predicted_cls = [1 if i > 0.5 else 0 for i in y_predicted]\n",
    "        return np.array(y_predicted_cls)\n",
    "   \n",
    "# Testing\n",
    "if __name__ == \"__main__\":\n",
    "   \n",
    "    def accuracys(y_true, y_pred):\n",
    "        accuracy = np.sum(y_true == y_pred) / len(y_true)\n",
    "        return accuracy\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc = datasets.load_breast_cancer()\n",
    "X, y = bc.data, bc.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0,stratify=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.LogisticRegressions'>\n",
      "The accuracy of this LogisticRegression model 97.37 %\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train=scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "regressor = LogisticRegressions(learning_rate=0.1, iterations=1000)\n",
    "print(type(regressor))\n",
    "regressor.fit(X_train, y_train)\n",
    "predictions = regressor.predict(X_test)\n",
    "\n",
    "print(\"The accuracy of this LogisticRegression model\", round(accuracys(y_test, predictions) *100,2),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename'])"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "dataset = datasets.load_breast_cancer()\n",
    "dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>0.05623</td>\n",
       "      <td>...</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>0.05533</td>\n",
       "      <td>...</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.05648</td>\n",
       "      <td>...</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07016</td>\n",
       "      <td>...</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>0.05884</td>\n",
       "      <td>...</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0          17.99         10.38          122.80     1001.0          0.11840   \n",
       "1          20.57         17.77          132.90     1326.0          0.08474   \n",
       "2          19.69         21.25          130.00     1203.0          0.10960   \n",
       "3          11.42         20.38           77.58      386.1          0.14250   \n",
       "4          20.29         14.34          135.10     1297.0          0.10030   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "564        21.56         22.39          142.00     1479.0          0.11100   \n",
       "565        20.13         28.25          131.20     1261.0          0.09780   \n",
       "566        16.60         28.08          108.30      858.1          0.08455   \n",
       "567        20.60         29.33          140.10     1265.0          0.11780   \n",
       "568         7.76         24.54           47.92      181.0          0.05263   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0             0.27760         0.30010              0.14710         0.2419   \n",
       "1             0.07864         0.08690              0.07017         0.1812   \n",
       "2             0.15990         0.19740              0.12790         0.2069   \n",
       "3             0.28390         0.24140              0.10520         0.2597   \n",
       "4             0.13280         0.19800              0.10430         0.1809   \n",
       "..                ...             ...                  ...            ...   \n",
       "564           0.11590         0.24390              0.13890         0.1726   \n",
       "565           0.10340         0.14400              0.09791         0.1752   \n",
       "566           0.10230         0.09251              0.05302         0.1590   \n",
       "567           0.27700         0.35140              0.15200         0.2397   \n",
       "568           0.04362         0.00000              0.00000         0.1587   \n",
       "\n",
       "     mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "0                   0.07871  ...          17.33           184.60      2019.0   \n",
       "1                   0.05667  ...          23.41           158.80      1956.0   \n",
       "2                   0.05999  ...          25.53           152.50      1709.0   \n",
       "3                   0.09744  ...          26.50            98.87       567.7   \n",
       "4                   0.05883  ...          16.67           152.20      1575.0   \n",
       "..                      ...  ...            ...              ...         ...   \n",
       "564                 0.05623  ...          26.40           166.10      2027.0   \n",
       "565                 0.05533  ...          38.25           155.00      1731.0   \n",
       "566                 0.05648  ...          34.12           126.70      1124.0   \n",
       "567                 0.07016  ...          39.42           184.60      1821.0   \n",
       "568                 0.05884  ...          30.37            59.16       268.6   \n",
       "\n",
       "     worst smoothness  worst compactness  worst concavity  \\\n",
       "0             0.16220            0.66560           0.7119   \n",
       "1             0.12380            0.18660           0.2416   \n",
       "2             0.14440            0.42450           0.4504   \n",
       "3             0.20980            0.86630           0.6869   \n",
       "4             0.13740            0.20500           0.4000   \n",
       "..                ...                ...              ...   \n",
       "564           0.14100            0.21130           0.4107   \n",
       "565           0.11660            0.19220           0.3215   \n",
       "566           0.11390            0.30940           0.3403   \n",
       "567           0.16500            0.86810           0.9387   \n",
       "568           0.08996            0.06444           0.0000   \n",
       "\n",
       "     worst concave points  worst symmetry  worst fractal dimension  target  \n",
       "0                  0.2654          0.4601                  0.11890       0  \n",
       "1                  0.1860          0.2750                  0.08902       0  \n",
       "2                  0.2430          0.3613                  0.08758       0  \n",
       "3                  0.2575          0.6638                  0.17300       0  \n",
       "4                  0.1625          0.2364                  0.07678       0  \n",
       "..                    ...             ...                      ...     ...  \n",
       "564                0.2216          0.2060                  0.07115       0  \n",
       "565                0.1628          0.2572                  0.06637       0  \n",
       "566                0.1418          0.2218                  0.07820       0  \n",
       "567                0.2650          0.4087                  0.12400       0  \n",
       "568                0.0000          0.2871                  0.07039       1  \n",
       "\n",
       "[569 rows x 31 columns]"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df =pd.DataFrame(dataset.data, columns =dataset.feature_names)\n",
    "df['target'] =dataset.target\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = dataset.data, dataset.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0,stratify=y)\n",
    "scaler = StandardScaler()\n",
    "X_train=scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model =LogisticRegression()\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_Predictions =model.predict(X_test)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm =confusion_matrix(y_test,Y_Predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(5,5))\n",
    "sns.heatmap(cm,annot=True)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Truth')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of this LogisticRegression model with sklearn is 98.25 % \n"
     ]
    }
   ],
   "source": [
    "accuracy = (Y_Predictions == y_test).sum()/len(y_test)\n",
    "print(f\"The accuracy of this LogisticRegression model with sklearn is {round(accuracy * 100,2)} % \")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8fac594bfae6525c0c41b4041d2d72effa188cc8ead05f81b1fab2bb098927fb"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
